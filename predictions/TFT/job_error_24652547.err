GpuFreq=control_disabled
[I 2024-11-11 10:58:44,038] A new study created in memory with name: no-name-1fed7c27-e414-4cf4-ad76-639eca5a0d7c
/pfs/data5/home/tu/tu_tu/tu_zxoul27/micromamba/envs/power/lib/python3.12/site-packages/torch/random.py:167: UserWarning: CUDA reports that you have 2 available devices, and you have used fork_rng without explicitly specifying which devices are being used. For safety, we initialize *every* CUDA device by default, which can be quite slow if you have a lot of CUDAs. If you know that you are only making use of a few CUDA devices, set the environment variable CUDA_VISIBLE_DEVICES or the 'devices' keyword argument of fork_rng with the set of devices you are actually using. For example, if you are using CPU only, set device.upper()_VISIBLE_DEVICES= or devices=[]; if you are using device 0 only, set CUDA_VISIBLE_DEVICES=0 or devices=[0].  To initialize all devices and suppress this warning, set the 'devices' keyword argument to `range(torch.cuda.device_count())`.
  warnings.warn(message)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
SLURM auto-requeueing enabled. Setting signal handlers.
Metric train_loss improved. New best score: 0.770
Metric train_loss improved by 0.566 >= min_delta = 0.0. New best score: 0.204
Metric train_loss improved by 0.014 >= min_delta = 0.0. New best score: 0.190
Metric train_loss improved by 0.069 >= min_delta = 0.0. New best score: 0.121
Metric train_loss improved by 0.055 >= min_delta = 0.0. New best score: 0.066
