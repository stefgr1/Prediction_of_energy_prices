batch_size: 16
dropout: 0.1604088464958991
hidden_dim: 80
input_chunk_length: 77
learning_rate: 7.707271665959132e-06
n_layers: 2
training_length: 82
